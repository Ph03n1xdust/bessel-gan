from typing import Any, Callable, Sequence, Optional
import jax
import jax.numpy as jnp
import optax
from besselgan.utilities import *
from ase import Atoms
from collections import defaultdict
import numpy as np


def make_atomic_number_map(atoms_arr: Sequence[Atoms]):
    """Creates a mapping between atomic numbers and internal type indices.

    Args:
        atoms_arr: The sequence of Atoms objects used to create the map.

    Returns:
        A dictionary mapping the atomic numbers to internal type indices.
    """
    atomtypes = set()
    for atoms in atoms_arr:
        for anum in atoms.get_atomic_numbers():
            atomtypes.add(anum)

    result = {}
    for i, anum in enumerate(sorted(atomtypes)):
        result[anum] = i

    return result


def make_training_dict(
    atoms_arr: Sequence[Atoms], descriptor_method: Callable, anum_map: dict[int, int]
):
    """Generates a dictionary holding possible "real descriptors" indexed by
    the atom types

    Args:
        atoms_arr: The sequence of Atoms objects to take descriptors from.
        descriptor_method: The method used to generate descriptors.
        anum_map: Mapping between atomic numbers and internal type indices.

    Returns:
        A dictionary of the list of possible "real descriptors" indexed by
        internal types.
    """
    result = defaultdict(list)

    for atoms in atoms_arr:
        pos = atoms.positions
        cell = atoms.cell[...]
        type = np.array([anum_map[anum] for anum in atoms.get_atomic_numbers()])
        descriptors = descriptor_method(pos, type, pos, type, cell)
        descriptors = descriptors.reshape((descriptors.shape[0], -1))

        for t, desc in zip(type, descriptors):
            result[t].append(desc)

    for t in result:
        result[t] = np.array(result[t])

    return result


def make_real_sampler(
    types: Sequence[int], n_batch: int, training_dict: dict[int, list]
):
    """Creates a sampler method which generates "real descriptors".

    Args:
        types: The internal types of the atoms for which "real descriptors" are
            expected.
        n_batch: The number of batches to generate the descriptors for.
        training_dict: The dictionary of descriptors generated by "make_training_dict".

    Returns:
        A sampler method which uses a PRNG key to generate "real descriptors"
            in the shape of (n_batch, len(types), len(descriptor)).
    """
    expected_shape = (n_batch, len(types), training_dict[0][0].shape[-1])
    type_positions = {}

    for t in types:
        type_positions[t] = np.where(types == t)[0]

    def sampler(rng):
        result = jnp.zeros(shape=expected_shape)

        for t in type_positions:
            rng, key = jax.random.split(rng)
            indices = type_positions[t]
            result = result.at[:, indices, :].set(
                jax.random.choice(key, training_dict[t], (n_batch, len(indices)))
            )

        return result, rng

    return sampler


def create_critic_step(
    critic: CriticModel,
    generator: GeneratorModel,
    descriptor_method: Callable,
    postprocess: Callable,
    n_batch: int,
    optimizer_crit: optax.GradientTransformation,
):
    """Creates the critic training step.

    Args:
        critic: The CriticModel to use.
        generator: The GeneratorModel to use.
        descriptor_method: The method used for descriptor generation.
        postprocess: The postrocessing method used to create the structures.
        n_batch: Number of batches in the training.
        optimizer_crit: The Optax optimizer object for critic training.

    Returns:
        A method which does one step in the critic training. It uses the
        current critic weights, the current generator weights, the critic
        optimizer state, a batch of "real" descriptors and a PRNG key
        respectively. It returns with the updated critic weights, critic
        optimizer state and PRNG key.
    """
    generate_batch_descriptor = create_generate_batch_descriptor(descriptor_method)
    generate_batch = jax.vmap(generator.apply, (None, 0), 0)
    criticize_batch = jax.vmap(critic.apply, (None, 0), 0)
    postprocess_batch = jax.vmap(postprocess)

    @jax.jit
    def critic_step(params_crit, params_gen, opt_state_crit, real_desc, rng):
        rng, key = jax.random.split(rng)
        fake_latent = jax.random.normal(key, (n_batch, generator.n_latent))
        intermediate = generate_batch(params_gen, fake_latent)
        (
            all_fake_pos,
            all_fake_type,
            fake_pos,
            fake_type,
            cell,
        ) = postprocess_batch(intermediate)

        fake_desc = generate_batch_descriptor(
            all_fake_pos, all_fake_type, fake_pos, fake_type, cell
        )
        fake_desc = jnp.reshape(fake_desc, (-1, fake_desc.shape[-1]))
        real_desc = jnp.reshape(real_desc, (-1, real_desc.shape[-1]))

        def loss(params):
            fake_preds = criticize_batch(params, fake_desc)
            real_preds = criticize_batch(params, real_desc)

            return jnp.mean(real_preds - fake_preds)

        grads = jax.grad(loss)(params_crit)
        updates, opt_state_crit = optimizer_crit.update(
            grads, opt_state_crit, params_crit
        )
        params_crit = optax.apply_updates(params_crit, updates)

        return params_crit, opt_state_crit, rng

    return critic_step


def create_gradient_penalty_step(
    critic: CriticModel,
    generator: GeneratorModel,
    descriptor_method: Callable,
    postprocess: Callable,
    n_batch: int,
    optimizer_crit: optax.GradientTransformation,
):
    """Creates the gradient-penalty step for the critic training.

    Args:
        critic: The CriticModel to use.
        generator: The GeneratorModel to use.
        descriptor_method: The method used for descriptor generation.
        postprocess: The postrocessing method used to create the structures.
        n_batch: Number of batches in the training.
        optimizer_crit: The Optax optimizer object for critic training.

    Returns:
        A method which does one step of enforcing the gradient-penalty of the
        critic. It uses the current critic weights, the current generator
        weights, the critic optimizer state, a batch of "real" descriptors and
        a PRNG key respectively. It returns with the updated critic weights,
        critic optimizer state and PRNG key.
    """
    generate_batch_descriptor = create_generate_batch_descriptor(descriptor_method)
    generate_batch = jax.vmap(generator.apply, (None, 0), 0)
    postprocess_batch = jax.vmap(postprocess)

    @jax.jit
    def critic_gp_step(params_crit, params_gen, opt_state_crit, real_desc, rng):
        rng, key = jax.random.split(rng)
        fake_latent = jax.random.normal(key, (n_batch, generator.n_latent))
        intermediate = generate_batch(params_gen, fake_latent)
        all_fake_pos, all_fake_type, fake_pos, fake_type, cell = postprocess_batch(
            intermediate
        )

        fake_desc = generate_batch_descriptor(
            all_fake_pos, all_fake_type, fake_pos, fake_type, cell
        )
        fake_desc = jnp.reshape(fake_desc, (-1, fake_desc.shape[-1]))

        rng, key = jax.random.split(rng)
        weights = jax.random.uniform(key, shape=(fake_desc.shape[0], 1))
        mid_descriptors = real_desc * weights + fake_desc * (1 - weights)

        def loss(params):
            def criticize(descriptor):
                return critic.apply(params, descriptor)

            critic_grad = jax.grad(criticize)
            critic_grad_batch = jax.vmap(critic_grad)
            grad_norms = jnp.linalg.norm(critic_grad_batch(mid_descriptors), axis=1)

            return jnp.mean((grad_norms - 1) ** 2)

        grads = jax.grad(loss)(params_crit)
        updates, opt_state_crit = optimizer_crit.update(grads, opt_state_crit)
        params_crit = optax.apply_updates(params_crit, updates)

        return params_crit, opt_state_crit, rng

    return critic_gp_step


def create_generator_step(
    critic: CriticModel,
    generator: GeneratorModel,
    descriptor_method: Callable,
    postprocess: Callable,
    n_batch: int,
    optimizer_gen: optax.GradientTransformation,
):
    """Creates the generator training step.

    Args:
        critic: The CriticModel to use.
        generator: The GeneratorModel to use.
        descriptor_method: The method used for descriptor generation.
        postprocess: The postrocessing method used to create the structures.
        n_batch: Number of batches in the training.
        optimizer_gen: The Optax optimizer object for generator training.

    Returns:
        A method which does one step of the generator training. It uses the
        current critic weights, the current generator weights, the generator
        optimizer state and a PRNG key respectively.
        It returns with the updated generator weights, generator optimizer
        state and PRNG key.
    """
    generate_batch_descriptor = create_generate_batch_descriptor(descriptor_method)
    generate_batch = jax.vmap(generator.apply, (None, 0), 0)
    criticize_batch = jax.vmap(critic.apply, (None, 0), 0)
    postprocess_batch = jax.vmap(postprocess)

    @jax.jit
    def generator_step(params_crit, params_gen, opt_state_gen, rng):
        rng, key = jax.random.split(rng)
        fake_latent = jax.random.normal(key, (n_batch, generator.n_latent))

        def loss(params):
            intermediate = generate_batch(params, fake_latent)
            all_fake_pos, all_fake_type, fake_pos, fake_type, cell = postprocess_batch(
                intermediate
            )

            fake_desc = generate_batch_descriptor(
                all_fake_pos, all_fake_type, fake_pos, fake_type, cell
            )
            fake_desc = jnp.reshape(fake_desc, (-1, fake_desc.shape[-1]))
            fake_preds = criticize_batch(params_crit, fake_desc)
            return jnp.mean(fake_preds)

        grads = jax.grad(loss)(params_gen)
        updates, opt_state_gen = optimizer_gen.update(grads, opt_state_gen)
        params_gen = optax.apply_updates(params_gen, updates)

        return params_gen, opt_state_gen, rng

    return generator_step
